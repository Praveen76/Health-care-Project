rm(list=ls())
setwd("C:/R/Driftter/Project")


Needed <- c("tm", "RColorBrewer", "ggplot2", "wordcloud", "doSNOW","slam","magrittr","e1071","xgboost",
  "syuzhet", "stringr","plyr","dplyr","Matrix","data.table","caret","cluster", "igraph", "fpc","mlr")

# install.packages(Needed, dependencies=TRUE)#Install Packages

lapply(Needed,require,character.only=TRUE) #Load Libraries
rm(Needed)

data <- fread("C:/R/Projects/Text Classification - HealthCare/TextClassification_Data.csv", sep=",", stringsAsFactors = FALSE)
class(data)
str(data)

##############EDA############
#Understanding of Data
head(data)
dim(data)
str(data)
colSums(is.na(data)) #Check missing values

# cat("The number of dupicated rows are", nrow(data)-nrow(unique(data)))
############Feature Engineering ########
# 1.UniVariate Analysis
str(data)
str(data$previous_appointment)
table(data$previous_appointment)

data$previous_appointment[data$previous_appointment %in% c("YES","yes","Yes")]="Yes"
data$previous_appointment[data$previous_appointment %in% c("No","NO","")]="No"
table(data$previous_appointment)

str(data)
#Another way of merging two variables
data$categories =  mapvalues(data$categories,from = c("asK_A_DOCTOR","mISCELLANEOUS","JUNK"),
                             to = c("ASK_A_DOCTOR","MISCELLANEOUS","MISCELLANEOUS"))
data$sub_categories = mapvalues(data$sub_categories, from = c("mEDICATION RELATED","JUNK"),
                                to = c("MEDICATION RELATED","OTHERS"))
# summarizeColumns(data) #Summarize values

unique(data$categories) #Unique levels in categories variable

table(data$categories)  #tabulate the number of occurances of each type


data[data$sub_categories=="CHANGE OF PHARMACY","SUMMARY"]
data[data$sub_categories=="CHANGE OF HOSPITAL","SUMMARY"]
data[data$sub_categories=="CHANGE OF PROVIDER","SUMMARY"]
data[data$sub_categories=="QUERY ON CURRENT APPOINTMENT","SUMMARY"]
data[data$sub_categories=="OTHERS","SUMMARY"]
data[data$sub_categories=="JUNK","SUMMARY"]
data[data$sub_categories=="OTHERS" & data$SUMMARY=="Phone Note","fileid" ]


data$categories<- as.factor(data$categories)
data$sub_categories<- as.factor(data$sub_categories)
data$previous_appointment<- as.factor(data$previous_appointment)
data$fileid<- as.numeric(data$fileid)

table(data$sub_categories)

##Analyzing categorical data with proportion
prop.table(table(data$categories))*100
prop.table(table(data$sub_categories))*100
prop.table(table(data$previous_appointment))*100

data$nchar <- as.numeric(nchar(data$DATA))
str(data)
data$nwords <- as.numeric(str_count(data$DATA, "\\S+"))


# histogram 
hist(data$nchar, main = "Histogram",
     xlab = "number of characters in data$DATA")

# histogram, Visulaize words length
hist(data$nwords, main = "Histogram",
     xlab = "number of words in data$DATA")



#Visualize categories and subcategories
# ggplot draws a blank plot with the aesthetics you give it
ggplot(data = data, aes(x = categories)) +
  geom_bar() + #draw the bars for a bar chart
  ggtitle("categories") +
  xlab("")
  
ggplot(data = data, aes(x = sub_categories)) +
  geom_bar() + #draw the bars for a bar chart
  ggtitle("Sub_categories") +
  theme(axis.text.x = element_text(color = "purple",angle = 45 ,size=7))+
  xlab("")


#Visualize words by their categories
ggplot(data, aes(nwords,fill=categories))+
  geom_histogram(binwidth = 6)+
  ggtitle("Visualize words by their categories")

#Visualize words by their sub_categories
ggplot(data, aes(nwords,fill=sub_categories))+
  geom_histogram(binwidth = 6)+
  ggtitle("Visualize words by
their subcategories")

#############Chi_Square Test#######
# Now we can use a chi-square test to see if the proportion of responses across the two questions Categories and Sub_categories? are different from chance.
chisq.test(data$categories, 
           data$sub_categories)

# Let's quickly vizualize this difference.

ggplot(data, aes(x = categories, 
                       fill = sub_categories)) + 
  geom_bar(position = "dodge")



# Now we can use a chi-square test to see if the proportion of responses across the two questions  ie.Categories and previous appointment? are different from chance.
chisq.test(data$categories, 
           data$previous_appointment)
# Let's quickly vizualize this difference.

ggplot(data, aes(x = categories, 
                 fill = previous_appointment)) + 
  geom_bar(position = "dodge")

str(data)
names(data) ##Check Column names of data

data$DATA[1]
data$DATA[7]

data1<- data$DATA #Backup of original data

###Convert in to Corpus
txt_corpus <- Corpus(VectorSource(data$DATA)) 
writeLines(as.character(txt_corpus[1])) #Analyze
writeLines(as.character(txt_corpus[3])) #Analyze

#Time the code execution start
start.time<-Sys.time()
#Create a PARLLEL SOCKET Cluster
cl<-makeCluster(4, type="PSOCK")
registerDoSNOW(cl)

for (j in seq(txt_corpus)) {
  txt_corpus[[j]] <- gsub("[a-z][\\\\]", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\}", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub(";", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\d+", " ", txt_corpus[[j]]) #Replace numbers with blank space
  txt_corpus[[j]] <- gsub("\\|", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("xxxx-xxxx", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\|", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\u2028", " ", txt_corpus[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}

stopCluster(cl)
total.time<- Sys.time()- start.time
total.time

writeLines(as.character(txt_corpus[1])) #Analyze
writeLines(as.character(txt_corpus[3]))

##Text cleaning##
txt <- tm_map(txt_corpus, content_transformer(tolower))

##Remove unnecessary words
txt <- tm_map(txt, removeWords,c("hydrocodoneacetaminophen","xxxxxxx","hydrocodoneacetaminophen"))
txt <- tm_map(txt, removeWords,"phonexxxxxxx")
txt <- tm_map(txt, removeWords,c("xxxxxxx","xxx"))

txt <- tm_map(txt, stripWhitespace)
txt <- tm_map(txt, removePunctuation)
txt <- tm_map(txt, removeWords, stopwords("english"))
txt <- tm_map(txt, stemDocument, language="english")
txt <- tm_map(txt, removeNumbers)

writeLines(as.character(txt[1])) #Analyze
txt <- tm_map(txt, removeWords, c("cs", "atparb", "hrod","rn","b","ansiftnbjfonttblf","margbsxn","margrsxn"))
writeLines(as.character(txt[1])) #Analyze
writeLines(as.character(txt[3]))

# # Remove additional words

txt <- tm_map(txt, removeWords,c("hydrocodoneacetaminophen","xxxxxxx","hydrocodoneacetaminophen"))
txt <- tm_map(txt, removeWords,c("xxxxxxx","xxx","phonexxxxxxx"))

##Convert in to DTM
#use the tf-idf(term frequency-inverse document frequency) instead of the frequencies of the term as entries
# tf-idf measures the relative importance of a word to a document.

dtm <- DocumentTermMatrix(txt, control = list(weighting = weightTfIdf))
dtm
dtm_review <- removeSparseTerms(dtm, 0.80)
dtm
inspect(dtm_review)
data <- cbind(data, as.matrix(dtm_review))

########## Frequent Terms and Associations#########3
##Frequent terms
findFreqTerms(dtm_review, lowfreq=200)
frequent<-findFreqTerms(dtm_review, lowfreq=100)
findFreqTerms(dtm_review, lowfreq=10)

##Find terms which occur atleast  500 times
# which words are associated with "call"?
#find associations (i.e., terms which correlate) with at least 0.8 correlation for the term call
findAssocs(dtm_review, "call", 0.5)
findAssocs(dtm_review, "followup", corlimit=0.4)


  ###Draw Word Cloud
  DTM_Matrix<- as.matrix(dtm_review)
  frequency <- colSums(DTM_Matrix)
  word_freqs <- data.frame(term = names(frequency), num = frequency)
  graphics.off()
  wordcloud(word_freqs$term, word_freqs$num,max.words = 200,scale = c(3,1) ,colors = brewer.pal(6,"Dark2"))
  
  # or 
freq = data.frame(sort(colSums(as.matrix(dtm_review)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))


######Hierarchal Clustering######
# First calculate distance between words & then cluster them according to similarity.


d <- dist(t(dtm_review), method="euclidian")   
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit 
plot(fit, hang=-1)   

##Dendogram
# Here, I have arbitrarily chosen to look at 3 clusters, as indicated by the red boxes
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=3)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=3, border="red") # draw dendogram with red borders around the 6 clusters   

###### K-means clustering######
# The k-means clustering method will attempt to cluster words into a specified number of groups (in this case 2), 
# such that the sum of squared distances between individual words and one of the group centers.
# You can change the number of groups you seek by changing the number specified within the kmeans() command.
library(fpc)   
d <- dist(t(dtm_review), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)  


# N-gram(1 gram) tokenization of the Corpus 
##################################################################################
OnegramTokenizer <- function(x) NGramTokenizer(x, 
                                               Weka_control(min = 1, max =1))
dtm_ngram <- DocumentTermMatrix(txt, control = list(tokenize = OnegramTokenizer))
dtm_ngram <- removeSparseTerms(dtm, 0.8)
DTM_Matrix<- Matrix(as.matrix(dtm_ngram))
freq <- sort(colSums(DTM_Matrix), decreasing=TRUE)
wof <- data.frame(word=names(freq), freq=freq)

pl <- ggplot(subset(wof, freq > 250) ,aes(word, freq))
pl <- pl + geom_bar(stat="identity", fill="darkred", colour="blue")
pl + theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("Uni-Gram Frequency")

#Naive Bayes Model for Categories

set.seed(2017)
library(caTools)
split=sample.split(data$categories,SplitRatio = 2/3)
training_set<- subset(data,split==TRUE)
test_set<- subset(data,split==FALSE)

model = naiveBayes(as.matrix(training_set),as.factor(training_set$categories) ,laplace=1)

summary(model)
model$apriori
model$levels

saveRDS(model, "NaiveBayesmodel.rds") #Save Model
mod2 <- readRDS("NaiveBayesmodel.rds") #Load Model
identical(model, mod2, ignore.environment = TRUE) ##Check if both models are same

result = predict(mod2,as.matrix(test_set),type = 'class')

confusionMatrix(result,test_set$categories)


#Naive Bayes Model for Sub_categories
# Set seed
set.seed(2017)
library(caTools)
split1=sample.split(data$sub_categories,SplitRatio = 2/3)
training_set1<- subset(data,split1==TRUE)
test_set1<- subset(data,split1==FALSE)

model_1 = naiveBayes(as.matrix(training_set1),
                   as.factor(training_set1$sub_categories))
summary(model_1)
model_1$apriori

saveRDS(model_1, "NaiveBayesmodel_1.rds")##Save Model
mod3 <- readRDS("NaiveBayesmodel_1.rds") ##Load Model

result1 = predict(mod3,as.matrix(test_set1),type = 'class')

confusionMatrix(result1,test_set1$sub_categories)

table_subcategories<-table(result1,test_set1$sub_categories)
write.csv(table_subcategories,"table_subcategories_NaiveBayes.csv") #So we can see confusion matrix easily

ID<-test_set$ID

result2<-as.data.frame(cbind(ID,cbind(result,result1)))
names(result2) <-c("ID","categories","sub_categories")
result2



identical(model_1, mod3, ignore.environment = TRUE) ##Check if both models are same
