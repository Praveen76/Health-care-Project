rm(list=ls())
setwd("C:/R/Projects/Text Classification - HealthCare")


Needed <- c("tm", "RColorBrewer", "ggplot2", "wordcloud", "doSNOW","slam","magrittr","e1071","xgboost",
  "syuzhet", "stringr","plyr","dplyr","Matrix","data.table","caret","cluster", "igraph", "fpc","mlr")

# install.packages(Needed, dependencies=TRUE)#Install Packages

lapply(Needed,require,character.only=TRUE) #Load Libraries
rm(Needed)

data <- fread("C:/R/Projects/Text Classification - HealthCare/TextClassification_Data.csv", sep=",", stringsAsFactors = FALSE)
class(data)
str(data)

##############EDA############
#Understanding of Data
head(data)
dim(data)
str(data)
colSums(is.na(data)) #Check missing values

# cat("The number of dupicated rows are", nrow(data)-nrow(unique(data)))
############Feature Engineering ########
# 1.UniVariate Analysis
str(data)
str(data$previous_appointment)
table(data$previous_appointment)

data$previous_appointment[data$previous_appointment %in% c("YES","yes","Yes")]="Yes"
data$previous_appointment[data$previous_appointment %in% c("No","NO","")]="No"
table(data$previous_appointment)

str(data)
#Another way of merging two variables
data$categories =  mapvalues(data$categories,from = c("asK_A_DOCTOR","mISCELLANEOUS","JUNK"),
                             to = c("ASK_A_DOCTOR","MISCELLANEOUS","MISCELLANEOUS"))
data$sub_categories = mapvalues(data$sub_categories, from = c("mEDICATION RELATED","JUNK"),
                                to = c("MEDICATION RELATED","OTHERS"))
summarizeColumns(data) #Summarize values

table(data$categories)

data[data$sub_categories=="CHANGE OF PHARMACY","SUMMARY"]
data[data$sub_categories=="CHANGE OF HOSPITAL","SUMMARY"]
data[data$sub_categories=="CHANGE OF PROVIDER","SUMMARY"]
data[data$sub_categories=="QUERY ON CURRENT APPOINTMENT","SUMMARY"]
data[data$sub_categories=="OTHERS","SUMMARY"]
data[data$sub_categories=="JUNK","SUMMARY"]
data[data$sub_categories=="OTHERS" & data$SUMMARY=="Phone Note","fileid" ]


data$categories<- as.factor(data$categories)
data$sub_categories<- as.factor(data$sub_categories)
data$previous_appointment<- as.factor(data$previous_appointment)
data$fileid<- as.numeric(data$fileid)

table(data$sub_categories)

data$nchar <- as.numeric(nchar(data$DATA))
str(data)
data$nwords <- as.numeric(str_count(data$DATA, "\\S+"))

hist(data$nwords) # Visulaize words length

#Visualize words by their categories
ggplot(data, aes(nwords,fill=categories))+
  geom_histogram(binwidth = 6)

#Visualize words by their sub_categories
ggplot(data, aes(nwords,fill=sub_categories))+
  geom_histogram(binwidth = 6)


str(data)
names(data) ##Check Column names of data

table(data$sub_categories)
data$DATA[1]
data$DATA[7]

data1<- data$DATA #Backup of original data

###Convert in to Corpus
txt_corpus <- Corpus(VectorSource(data$DATA)) 
writeLines(as.character(txt_corpus[1])) #Analyze
writeLines(as.character(txt_corpus[3])) #Analyze

#Time the code execution start
start.time<-Sys.time()
#Create a PARLLEL SOCKET Cluster
cl<-makeCluster(4, type="PSOCK")
registerDoSNOW(cl)

for (j in seq(txt_corpus)) {
  txt_corpus[[j]] <- gsub("[a-z][\\\\]", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\}", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub(";", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\d+", " ", txt_corpus[[j]]) #Replace numbers with blank space
  txt_corpus[[j]] <- gsub("\\|", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("xxxx-xxxx", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\\|", " ", txt_corpus[[j]])
  txt_corpus[[j]] <- gsub("\u2028", " ", txt_corpus[[j]])  # This is an ascii character that did not translate, so it had to be removed.
}

stopCluster(cl)
total.time<- Sys.time()- start.time
total.time

writeLines(as.character(txt_corpus[1])) #Analyze
writeLines(as.character(txt_corpus[3]))

##Text cleaning##
txt <- tm_map(txt_corpus, content_transformer(tolower))

##Remove unnecessary words
txt <- tm_map(txt, removeWords,c("hydrocodoneacetaminophen","xxxxxxx","hydrocodoneacetaminophen"))
txt <- tm_map(txt, removeWords,"phonexxxxxxx")
txt <- tm_map(txt, removeWords,c("xxxxxxx","xxx"))

txt <- tm_map(txt, stripWhitespace)
txt <- tm_map(txt, removePunctuation)
txt <- tm_map(txt, removeWords, stopwords("english"))
txt <- tm_map(txt, stemDocument, language="english")
txt <- tm_map(txt, removeNumbers)

writeLines(as.character(txt[1])) #Analyze
txt <- tm_map(txt, removeWords, c("cs", "atparb", "hrod","rn","b","ansiftnbjfonttblf","margbsxn","margrsxn"))
writeLines(as.character(txt[1])) #Analyze
writeLines(as.character(txt[3]))

# # Remove additional words

txt <- tm_map(txt, removeWords,c("hydrocodoneacetaminophen","xxxxxxx","hydrocodoneacetaminophen"))
txt <- tm_map(txt, removeWords,c("xxxxxxx","xxx","phonexxxxxxx"))

##Convert in to DTM
#use the tf-idf(term frequency-inverse document frequency) instead of the frequencies of the term as entries
# tf-idf measures the relative importance of a word to a document.

dtm <- DocumentTermMatrix(txt, control = list(weighting = weightTfIdf))
dtm
dtm_review <- removeSparseTerms(dtm, 0.80)
dtm
inspect(dtm_review)
data <- cbind(data, as.matrix(dtm_review))

########## Frequent Terms and Associations#########3
##Frequent terms
findFreqTerms(dtm_review, lowfreq=200)
frequent<-findFreqTerms(dtm_review, lowfreq=100)
findFreqTerms(dtm_review, lowfreq=10)

##Find terms which occur atleast  500 times
# which words are associated with "call"?
#find associations (i.e., terms which correlate) with at least 0.8 correlation for the term call
findAssocs(dtm_review, "call", 0.5)
findAssocs(dtm_review, "followup", corlimit=0.4)


####### Dictionary
Dictionary<-DocumentTermMatrix(txt, list(dictionary = c("call", "appointment", "followup","patient","phone")))
inspect(Dictionary)

  ###Draw Word Cloud
  DTM_Matrix<- as.matrix(dtm_review)
  frequency <- colSums(DTM_Matrix)
  word_freqs <- data.frame(term = names(frequency), num = frequency)
  graphics.off()
  wordcloud(word_freqs$term, word_freqs$num,max.words = 200,scale = c(3,1) ,colors = brewer.pal(6,"Dark2"))
  
  # or 
freq = data.frame(sort(colSums(as.matrix(dtm_review)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))

# # Sentiment analysis

sentiment <- get_nrc_sentiment(data$DATA)
# data <- cbind(data,sentiment)

td<-data.frame(t(sentiment))
td_new <- data.frame(rowSums(td[2:7945]))
#The function rowSums computes column sums across rows for each level of a grouping variable.

#Transformation and  cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]

#Visualisation
qplot(sentiment, data=td_new2, weight=count, geom="bar",fill=sentiment)+ggtitle("Email sentiments")



######Hierarchal Clustering######
# First calculate distance between words & then cluster them according to similarity.


d <- dist(t(dtm_review), method="euclidian")   
fit <- hclust(d=d, method="complete")   # for a different look try substituting: method="ward.D"
fit 
plot(fit, hang=-1)   

##Dendogram
# Here, I have arbitrarily chosen to look at 3 clusters, as indicated by the red boxes
plot.new()
plot(fit, hang=-1)
groups <- cutree(fit, k=3)   # "k=" defines the number of clusters you are using   
rect.hclust(fit, k=3, border="red") # draw dendogram with red borders around the 6 clusters   

###### K-means clustering######
# The k-means clustering method will attempt to cluster words into a specified number of groups (in this case 2), 
# such that the sum of squared distances between individual words and one of the group centers.
# You can change the number of groups you seek by changing the number specified within the kmeans() command.
library(fpc)   
d <- dist(t(dtm_review), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)  


# N-gram(1 gram) tokenization of the Corpus 
##################################################################################
OnegramTokenizer <- function(x) NGramTokenizer(x, 
                                               Weka_control(min = 1, max =1))
dtm_ngram <- DocumentTermMatrix(txt, control = list(tokenize = OnegramTokenizer))
dtm_ngram <- removeSparseTerms(dtm, 0.8)
DTM_Matrix<- Matrix(as.matrix(dtm_ngram))
freq <- sort(colSums(DTM_Matrix), decreasing=TRUE)
wof <- data.frame(word=names(freq), freq=freq)

pl <- ggplot(subset(wof, freq > 250) ,aes(word, freq))
pl <- pl + geom_bar(stat="identity", fill="darkred", colour="blue")
pl + theme(axis.text.x=element_text(angle=45, hjust=1)) + ggtitle("Uni-Gram Frequency")

#Naive Bayes Model for Categories

set.seed(2017)
library(caTools)
split=sample.split(data$categories,SplitRatio = 2/3)
training_set<- subset(data,split==TRUE)
test_set<- subset(data,split==FALSE)

model = naiveBayes(as.matrix(training_set),as.factor(training_set$categories) ,laplace=1)

summary(model)
model$apriori
model$levels

saveRDS(model, "NaiveBayesmodel.rds") #Save Model
mod2 <- readRDS("NaiveBayesmodel.rds") #Load Model
identical(model, mod2, ignore.environment = TRUE) ##Check if both models are same

result = predict(mod2,as.matrix(test_set),type = 'class')

confusionMatrix(result,test_set$sub_categories)


#Naive Bayes Model for Sub_categories
# Set seed
set.seed(2017)
library(caTools)
split1=sample.split(data$sub_categories,SplitRatio = 2/3)
training_set1<- subset(data,split1==TRUE)
test_set1<- subset(data,split1==FALSE)

model_1 = naiveBayes(as.matrix(training_set1),
                   as.factor(training_set1$sub_categories))
summary(model_1)
model_1$apriori

saveRDS(model_1, "NaiveBayesmodel_1.rds")##Save Model
mod3 <- readRDS("NaiveBayesmodel_1.rds") ##Load Model

result1 = predict(mod3,as.matrix(test_set1),type = 'class')

confusionMatrix(result1,test_set1$sub_categories)

ID<-test_set$ID

result2<-as.data.frame(cbind(ID,cbind(result,result1)))
names(result2) <-c("ID","categories","sub_categories")
result2



identical(model_1, mod3, ignore.environment = TRUE) ##Check if both models are same
